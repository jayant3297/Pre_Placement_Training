{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOIvMpe6iPYaVt09AdXupX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayant3297/Pre_Placement_Training/blob/main/12_July_Hadoop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop."
      ],
      "metadata": {
        "id": "7wczTmvW-M2H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Tjq_CmC0st_"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "def list_hdfs_files_and_directories(hdfs_path):\n",
        "    command = f\"hadoop fs -ls {hdfs_path}\"\n",
        "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
        "    output = result.stdout.strip()\n",
        "    files_and_directories = [line.split()[-1] for line in output.split('\\n')[1:]]\n",
        "\n",
        "    return files_and_directories\n",
        "\n",
        "# Example usage\n",
        "hdfs_directory = \"/user/data\"\n",
        "contents = list_hdfs_files_and_directories(hdfs_directory)\n",
        "print(f\"Contents of {hdfs_directory}:\")\n",
        "for item in contents:\n",
        "    print(item)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory."
      ],
      "metadata": {
        "id": "nqIvhlCN-Txf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def analyze_datanode_storage_utilization(nn_host, nn_port):\n",
        "    dn_url = f\"http://{nn_host}:{nn_port}/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo\"\n",
        "    dn_response = requests.get(dn_url)\n",
        "    dn_data = dn_response.json()\n",
        "    dn_info = dn_data['beans'][0]['LiveNodes']\n",
        "\n",
        "    storage_utilization = {}\n",
        "    for node_id, node_data in dn_info.items():\n",
        "        capacity = node_data['capacity']\n",
        "        used = node_data['used']\n",
        "        utilization = (used / capacity) * 100\n",
        "        storage_utilization[node_id] = utilization\n",
        "\n",
        "    highest_utilization_node = max(storage_utilization, key=storage_utilization.get)\n",
        "    lowest_utilization_node = min(storage_utilization, key=storage_utilization.get)\n",
        "\n",
        "    print(\"Storage Utilization:\")\n",
        "    for node, utilization in storage_utilization.items():\n",
        "        print(f\"Node: {node}, Utilization: {utilization}%\")\n",
        "    print(f\"Highest Utilization: Node: {highest_utilization_node}, Utilization: {storage_utilization[highest_utilization_node]}%\")\n",
        "    print(f\"Lowest Utilization: Node: {lowest_utilization_node}, Utilization: {storage_utilization[lowest_utilization_node]}%\")\n",
        "\n",
        "# Example usage\n",
        "namenode_host = \"localhost\"\n",
        "namenode_port = 50070\n",
        "analyze_datanode_storage_utilization(namenode_host, namenode_port)\n"
      ],
      "metadata": {
        "id": "OlmfMoTn-TL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach.\n"
      ],
      "metadata": {
        "id": "UdFoqrdZ-e6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def extract_top_words(file_path, top_n):\n",
        "    words = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            words += re.findall(r'\\w+', line.lower())\n",
        "\n",
        "    word_counts = Counter(words)\n",
        "    top_words = word_counts.most_common(top_n)\n",
        "\n",
        "    print(f\"Top {top_n} most frequent words:\")\n",
        "    for word, count in top_words:\n",
        "        print(f\"{word}: {count}\")\n",
        "\n",
        "# Example usage\n",
        "text_file_path = \"large_text_file.txt\"\n",
        "top_n = 10\n",
        "extract_top_words(text_file_path, top_n)\n"
      ],
      "metadata": {
        "id": "h9-lCPCw-jgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API.\n"
      ],
      "metadata": {
        "id": "G1rmR0v6-2km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def check_hadoop_cluster_health(nn_host, nn_port):\n",
        "    nn_url = f\"http://{nn_host}:{nn_port}/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus\"\n",
        "    nn_response = requests.get(nn_url)\n",
        "    nn_data = nn_response.json()\n",
        "    nn_status = nn_data['beans'][0]['State']\n",
        "\n",
        "    dn_url = f\"http://{nn_host}:{nn_port}/jmx?qry=Hadoop:service=NameNode,name=DataNodeInfo\"\n",
        "    dn_response = requests.get(dn_url)\n",
        "    dn_data = dn_response.json()\n",
        "    dn_status = dn_data['beans'][0]['LiveNodes']\n",
        "\n",
        "    print(f\"NameNode Status: {nn_status}\")\n",
        "    print(f\"DataNode Status: {dn_status}\")\n",
        "\n",
        "# Example usage\n",
        "namenode_host = \"localhost\"\n",
        "namenode_port = 50070\n",
        "check_hadoop_cluster_health(namenode_host, namenode_port)\n"
      ],
      "metadata": {
        "id": "ArVU885Z-3NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Develop a Python program that lists all the files and directories in a specific HDFS path."
      ],
      "metadata": {
        "id": "if5Wgn8W_BLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "def list_hdfs_files_and_directories(hdfs_path):\n",
        "    command = f\"hadoop fs -ls {hdfs_path}\"\n",
        "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
        "    output = result.stdout.strip()\n",
        "    files_and_directories = [line.split()[-1] for line in output.split('\\n')[1:]]\n",
        "\n",
        "    return files_and_directories\n",
        "\n",
        "# Example usage\n",
        "hdfs_directory = \"/user/data\"\n",
        "contents = list_hdfs_files_and_directories(hdfs_directory)\n",
        "print(f\"Contents of {hdfs_directory}:\")\n",
        "for item in contents:\n",
        "    print(item)\n"
      ],
      "metadata": {
        "id": "pndIjc18_DwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities."
      ],
      "metadata": {
        "id": "5jz6eVtx_JVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def analyze_datanode_storage_utilization(nn_host, nn_port):\n",
        "    dn_url = f\"http://{nn_host}:{nn_port}/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo\"\n",
        "    dn_response = requests.get(dn_url)\n",
        "    dn_data = dn_response.json()\n",
        "    dn_info = dn_data['beans'][0]['LiveNodes']\n",
        "\n",
        "    storage_utilization = {}\n",
        "    for node_id, node_data in dn_info.items():\n",
        "        capacity = node_data['capacity']\n",
        "        used = node_data['used']\n",
        "        utilization = (used / capacity) * 100\n",
        "        storage_utilization[node_id] = utilization\n",
        "\n",
        "    highest_utilization_node = max(storage_utilization, key=storage_utilization.get)\n",
        "    lowest_utilization_node = min(storage_utilization, key=storage_utilization.get)\n",
        "\n",
        "    print(\"Storage Utilization:\")\n",
        "    for node, utilization in storage_utilization.items():\n",
        "        print(f\"Node: {node}, Utilization: {utilization}%\")\n",
        "    print(f\"Highest Utilization: Node: {highest_utilization_node}, Utilization: {storage_utilization[highest_utilization_node]}%\")\n",
        "    print(f\"Lowest Utilization: Node: {lowest_utilization_node}, Utilization: {storage_utilization[lowest_utilization_node]}%\")\n",
        "\n",
        "# Example usage\n",
        "namenode_host = \"localhost\"\n",
        "namenode_port = 50070\n",
        "analyze_datanode_storage_utilization(namenode_host, namenode_port)\n"
      ],
      "metadata": {
        "id": "uXMZikSD_OQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output.\n"
      ],
      "metadata": {
        "id": "YpWwpx08_Tzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "def submit_hadoop_job(app_name, app_type, jar_path, class_name, input_path, output_path):\n",
        "    rm_url = \"http://<resourcemanager>:8088/ws/v1/cluster/apps\"\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    data = {\n",
        "        \"application-id\": \"application_123456789_0001\",\n",
        "        \"application-name\": app_name,\n",
        "        \"application-type\": app_type,\n",
        "        \"am-container-spec\": {\n",
        "            \"commands\": {\n",
        "                \"command\": f\"yarn jar {jar_path} {class_name} {input_path} {output_path}\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    response = requests.post(rm_url, headers=headers, json=data)\n",
        "    application_id = response.json()[\"app\"][\"id\"]\n",
        "\n",
        "    return application_id\n",
        "\n",
        "def monitor_job_progress(application_id):\n",
        "    rm_url = f\"http://<resourcemanager>:8088/ws/v1/cluster/apps/{application_id}\"\n",
        "\n",
        "    while True:\n",
        "        response = requests.get(rm_url)\n",
        "        status = response.json()[\"app\"][\"state\"]\n",
        "\n",
        "        if status == \"RUNNING\":\n",
        "            progress = response.json()[\"app\"][\"progress\"]\n",
        "            print(f\"Job {application_id} is running. Progress: {progress}%\")\n",
        "        elif status == \"FINISHED\":\n",
        "            print(f\"Job {application_id} finished successfully.\")\n",
        "            break\n",
        "        elif status in [\"FAILED\", \"KILLED\"]:\n",
        "            print(f\"Job {application_id} failed or was killed.\")\n",
        "            break\n",
        "\n",
        "        time.sleep(5)\n",
        "\n",
        "# Example usage\n",
        "application_name = \"WordCountJob\"\n",
        "application_type = \"MAPREDUCE\"\n",
        "jar_file_path = \"wordcount.jar\"\n",
        "main_class = \"com.example.WordCount\"\n",
        "input_path = \"/input\"\n",
        "output_path = \"/output\"\n",
        "app_id = submit_hadoop_job(application_name, application_type, jar_file_path, main_class, input_path, output_path)\n",
        "monitor_job_progress(app_id)\n"
      ],
      "metadata": {
        "id": "MhGL1LQV_W7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution."
      ],
      "metadata": {
        "id": "-KiXfN8v_ljK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "def submit_hadoop_job(app_name, app_type, jar_path, class_name, input_path, output_path, vcores, memory):\n",
        "    rm_url = \"http://<resourcemanager>:8088/ws/v1/cluster/apps\"\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    data = {\n",
        "        \"application-id\": \"application_123456789_0001\",\n",
        "        \"application-name\": app_name,\n",
        "        \"application-type\": app_type,\n",
        "        \"am-container-spec\": {\n",
        "            \"resource\": {\n",
        "                \"vCores\": vcores,\n",
        "                \"memory\": memory\n",
        "            },\n",
        "            \"commands\": {\n",
        "                \"command\": f\"yarn jar {jar_path} {class_name} {input_path} {output_path}\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    response = requests.post(rm_url, headers=headers, json=data)\n",
        "    application_id = response.json()[\"app\"][\"id\"]\n",
        "\n",
        "    return application_id\n",
        "\n",
        "def track_resource_usage(application_id):\n",
        "    rm_url = f\"http://<resourcemanager>:8088/ws/v1/cluster/apps/{application_id}/appattempts\"\n",
        "\n",
        "    while True:\n",
        "        response = requests.get(rm_url)\n",
        "        attempts = response.json()[\"appAttempts\"][\"appAttempt\"]\n",
        "\n",
        "        for attempt in attempts:\n",
        "            attempt_id = attempt[\"id\"]\n",
        "            resource_usage = attempt[\"allocatedResources\"]\n",
        "            vcores = resource_usage[\"vCores\"]\n",
        "            memory = resource_usage[\"memory\"]\n",
        "\n",
        "            print(f\"Attempt: {attempt_id}, vCores: {vcores}, Memory: {memory}\")\n",
        "\n",
        "        if attempts[-1][\"finalStatus\"] in [\"FAILED\", \"KILLED\"]:\n",
        "            print(f\"Job {application_id} failed or was killed.\")\n",
        "            break\n",
        "\n",
        "        time.sleep(5)\n",
        "\n",
        "# Example usage\n",
        "application_name = \"WordCountJob\"\n",
        "application_type = \"MAPREDUCE\"\n",
        "jar_file_path = \"wordcount.jar\"\n",
        "main_class = \"com.example.WordCount\"\n",
        "input_path = \"/input\"\n",
        "output_path = \"/output\"\n",
        "requested_vcores = 2\n",
        "requested_memory = 2048\n",
        "app_id = submit_hadoop_job(application_name, application_type, jar_file_path, main_class, input_path, output_path, requested_vcores, requested_memory)\n",
        "track_resource_usage(app_id)\n"
      ],
      "metadata": {
        "id": "ntklD83K_1Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time."
      ],
      "metadata": {
        "id": "z649jypXBVLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_mapreduce_job(input_path, output_path, split_size):\n",
        "    command = f\"hadoop jar mapreduce-job.jar com.example.MapReduceJob {input_path} {output_path} {split_size}\"\n",
        "    start_time = time.time()\n",
        "    subprocess.run(command, shell=True)\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "\n",
        "    return execution_time\n",
        "\n",
        "# Example usage\n",
        "input_file_path = \"/path/to/input/file.txt\"\n",
        "output_directory = \"/path/to/output\"\n",
        "split_sizes = [64, 128, 256, 512]\n",
        "\n",
        "for split_size in split_sizes:\n",
        "    print(f\"Running MapReduce job with split size: {split_size} MB\")\n",
        "    execution_time = run_mapreduce_job(input_file_path, output_directory, split_size)\n",
        "    print(f\"Execution time: {execution_time} seconds\\n\")\n"
      ],
      "metadata": {
        "id": "DzhWi8hvBWfm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}