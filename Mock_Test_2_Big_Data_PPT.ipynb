{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6oYQj01Mqias7ln4aIItZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayant3297/Pre_Placement_Training/blob/main/Mock_Test_2_Big_Data_PPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write an SQL query to find the second-highest salary from an \"Employees\" table"
      ],
      "metadata": {
        "id": "ZcbhppN7tfrN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRzzyo2ktRDc"
      },
      "outputs": [],
      "source": [
        "SELECT MAX(salary) AS second_highest_salary\n",
        "FROM Employees\n",
        "WHERE salary < (SELECT MAX(salary) FROM Employees)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a MapReduce program to calculate the word count of a given input text file."
      ],
      "metadata": {
        "id": "8KOWLOtRttae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mapper code\n",
        "\n",
        "import java.io.IOException;\n",
        "import org.apache.hadoop.io.IntWritable;\n",
        "import org.apache.hadoop.io.LongWritable;\n",
        "import org.apache.hadoop.io.Text;\n",
        "import org.apache.hadoop.mapred.MapReduceBase;\n",
        "import org.apache.hadoop.mapred.Mapper;\n",
        "import org.apache.hadoop.mapred.OutputCollector;\n",
        "import org.apache.hadoop.mapred.Reporter;\n",
        "\n",
        "public class WCMapper extends MapReduceBase implements Mapper<LongWritable,\n",
        "                                                Text, Text, IntWritable> {\n",
        "\n",
        "    // Map function\n",
        "    public void map(LongWritable key, Text value, OutputCollector<Text,\n",
        "                 IntWritable> output, Reporter rep) throws IOException\n",
        "    {\n",
        "\n",
        "        String line = value.toString();\n",
        "\n",
        "        // Splitting the line on spaces\n",
        "        for (String word : line.split(\" \"))\n",
        "        {\n",
        "            if (word.length() > 0)\n",
        "            {\n",
        "                output.collect(new Text(word), new IntWritable(1));\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "-TpQPgD0t0vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reducer\n",
        "\n",
        "import java.io.IOException;\n",
        "import java.util.Iterator;\n",
        "import org.apache.hadoop.io.IntWritable;\n",
        "import org.apache.hadoop.io.Text;\n",
        "import org.apache.hadoop.mapred.MapReduceBase;\n",
        "import org.apache.hadoop.mapred.OutputCollector;\n",
        "import org.apache.hadoop.mapred.Reducer;\n",
        "import org.apache.hadoop.mapred.Reporter;\n",
        "\n",
        "public class WCReducer extends MapReduceBase implements Reducer<Text,\n",
        "                                    IntWritable, Text, IntWritable> {\n",
        "\n",
        "    public void reduce(Text key, Iterator<IntWritable> value,\n",
        "                   OutputCollector<Text, IntWritable> output,\n",
        "                            Reporter rep) throws IOException\n",
        "    {\n",
        "\n",
        "        int count = 0;\n",
        "\n",
        "        // Counting the frequency of each words\n",
        "        while (value.hasNext())\n",
        "        {\n",
        "            IntWritable i = value.next();\n",
        "            count += i.get();\n",
        "        }\n",
        "\n",
        "        output.collect(key, new IntWritable(count));\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "T9oKbVc_vgwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main\n",
        "\n",
        "import java.io.IOException;\n",
        "import org.apache.hadoop.conf.Configured;\n",
        "import org.apache.hadoop.fs.Path;\n",
        "import org.apache.hadoop.io.IntWritable;\n",
        "import org.apache.hadoop.io.Text;\n",
        "import org.apache.hadoop.mapred.FileInputFormat;\n",
        "import org.apache.hadoop.mapred.FileOutputFormat;\n",
        "import org.apache.hadoop.mapred.JobClient;\n",
        "import org.apache.hadoop.mapred.JobConf;\n",
        "import org.apache.hadoop.util.Tool;\n",
        "import org.apache.hadoop.util.ToolRunner;\n",
        "\n",
        "public class WCDriver extends Configured implements Tool {\n",
        "\n",
        "    public int run(String args[]) throws IOException\n",
        "    {\n",
        "        if (args.length < 2)\n",
        "        {\n",
        "            System.out.println(\"Please give valid inputs\");\n",
        "            return -1;\n",
        "        }\n",
        "\n",
        "        JobConf conf = new JobConf(WCDriver.class);\n",
        "        FileInputFormat.setInputPaths(conf, new Path(args[0]));\n",
        "        FileOutputFormat.setOutputPath(conf, new Path(args[1]));\n",
        "        conf.setMapperClass(WCMapper.class);\n",
        "        conf.setReducerClass(WCReducer.class);\n",
        "        conf.setMapOutputKeyClass(Text.class);\n",
        "        conf.setMapOutputValueClass(IntWritable.class);\n",
        "        conf.setOutputKeyClass(Text.class);\n",
        "        conf.setOutputValueClass(IntWritable.class);\n",
        "        JobClient.runJob(conf);\n",
        "        return 0;\n",
        "    }\n",
        "\n",
        "    // Main Method\n",
        "    public static void main(String args[]) throws Exception\n",
        "    {\n",
        "        int exitCode = ToolRunner.run(new WCDriver(), args);\n",
        "        System.out.println(exitCode);\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "7hpGFvthvw6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a Spark program to count the number of occurrences of each word in a given text file."
      ],
      "metadata": {
        "id": "hZQsF0wMv-uE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Create SparkConf and SparkContext\n",
        "conf = SparkConf().setAppName(\"WordCount\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Load the text file\n",
        "text_file = sc.textFile(\"input_file.txt\")\n",
        "\n",
        "# Split the lines into words\n",
        "words = text_file.flatMap(lambda line: line.split(\" \"))\n",
        "\n",
        "# Count the occurrences of each word\n",
        "word_counts = words.countByValue()\n",
        "\n",
        "# Print the word counts\n",
        "for word, count in word_counts.items():\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "id": "sgrELK8Ev_a7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}