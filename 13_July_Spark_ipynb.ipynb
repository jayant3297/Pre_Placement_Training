{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPnodlEdxouQeyTw7hmW3z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayant3297/Pre_Placement_Training/blob/main/13_July_Spark_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFOvpB_WPLmB",
        "outputId": "f8d3856b-8932-495a-eff9-8fb86968cd5f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=3ab2ceb4d8fafe995575e1bd9d5799a358ff6b42f02bbfcdc2fbd88da1684285\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Working with RDDs:\n",
        "\n",
        "   a) Write a Python program to create an RDD from a local data source.\n",
        "\n",
        "   b) Implement transformations and actions on the RDD to perform data processing tasks.\n",
        "   \n",
        "   c) Analyze and manipulate data using RDD operations such as map, filter, reduce, or aggregate.\n"
      ],
      "metadata": {
        "id": "hnKByszaO7Qh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0CjdChgLsPq",
        "outputId": "00f80f2b-0ec1-443e-805f-d39ad92928dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD elements: [1, 2, 3, 4, 5]\n"
          ]
        }
      ],
      "source": [
        "#a)\n",
        "\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(\"local\", \"RDDExample\")\n",
        "\n",
        "# Create an RDD from a local data source (list)\n",
        "data = [1, 2, 3, 4, 5]\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Print the RDD elements\n",
        "print(\"RDD elements:\", rdd.collect())\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#b)\n",
        "\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(\"local\", \"RDDExample\")\n",
        "\n",
        "# Create an RDD from a local data source (list)\n",
        "data = [1, 2, 3, 4, 5]\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Transformations\n",
        "squared_rdd = rdd.map(lambda x: x ** 2)  # Square each element\n",
        "filtered_rdd = squared_rdd.filter(lambda x: x > 10)  # Filter elements greater than 10\n",
        "\n",
        "# Actions\n",
        "sum_result = filtered_rdd.reduce(lambda x, y: x + y)  # Sum all the elements\n",
        "\n",
        "# Print the RDD elements and the result of the action\n",
        "print(\"RDD elements:\", rdd.collect())\n",
        "print(\"Squared RDD elements:\", squared_rdd.collect())\n",
        "print(\"Filtered RDD elements:\", filtered_rdd.collect())\n",
        "print(\"Sum of filtered RDD elements:\", sum_result)\n",
        "\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzqAqA7uPxFz",
        "outputId": "92c59d68-f8e7-46e8-ba67-ee22023ec8fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD elements: [1, 2, 3, 4, 5]\n",
            "Squared RDD elements: [1, 4, 9, 16, 25]\n",
            "Filtered RDD elements: [16, 25]\n",
            "Sum of filtered RDD elements: 41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#c)\n",
        "\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(\"local\", \"RDDExample\")\n",
        "\n",
        "# Create an RDD from a local data source (list)\n",
        "data = [1, 2, 3, 4, 5]\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Map operation - Square each element\n",
        "squared_rdd = rdd.map(lambda x: x ** 2)\n",
        "\n",
        "# Filter operation - Select elements greater than 3\n",
        "filtered_rdd = squared_rdd.filter(lambda x: x > 3)\n",
        "\n",
        "# Reduce operation - Sum all the elements\n",
        "sum_result = filtered_rdd.reduce(lambda x, y: x + y)\n",
        "\n",
        "# Aggregate operation\n",
        "count = filtered_rdd.count()\n",
        "sum_all = filtered_rdd.sum()\n",
        "average = sum_all / count\n",
        "\n",
        "# Print the RDD elements and the results\n",
        "print(\"RDD elements:\", rdd.collect())\n",
        "print(\"Squared RDD elements:\", squared_rdd.collect())\n",
        "print(\"Filtered RDD elements:\", filtered_rdd.collect())\n",
        "print(\"Sum of filtered RDD elements:\", sum_result)\n",
        "print(\"Average of filtered RDD elements:\", average)\n",
        "\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "id": "S-LmGi7PP8-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Spark DataFrame Operations:\n",
        "\n",
        "   a) Write a Python program to load a CSV file into a Spark DataFrame.\n",
        "\n",
        "   b)Perform common DataFrame operations such as filtering, grouping, or joining.\n",
        "\n",
        "   c) Apply Spark SQL queries on the DataFrame to extract insights from the data.\n"
      ],
      "metadata": {
        "id": "lDv1BJ5tQRWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#a)\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()\n",
        "\n",
        "# Load a CSV file into a DataFrame\n",
        "df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Show the DataFrame schema and first few rows\n",
        "df.printSchema()\n",
        "df.show()\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "krWwbhVyQXbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#b)\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()\n",
        "\n",
        "# Load a CSV file into a DataFrame\n",
        "df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Filter operation - Select rows where age is greater than 30\n",
        "filtered_df = df.filter(df.age > 30)\n",
        "\n",
        "# GroupBy operation - Group by department and calculate average salary\n",
        "grouped_df = df.groupBy(\"department\").avg(\"salary\")\n",
        "\n",
        "# Join operation - Join two DataFrames based on a common column\n",
        "joined_df = df.join(other_df, df.common_column == other_df.common_column, \"inner\")\n",
        "\n",
        "# Show the results of the operations\n",
        "filtered_df.show()\n",
        "grouped_df.show()\n",
        "joined_df.show()\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "0CGMqC5aUbsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#c)\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()\n",
        "\n",
        "# Load a CSV file into a DataFrame\n",
        "df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Create a temporary view of the DataFrame\n",
        "df.createOrReplaceTempView(\"my_table\")\n",
        "\n",
        "# Run Spark SQL queries on the DataFrame\n",
        "result = spark.sql(\"SELECT * FROM my_table WHERE age > 30\")\n",
        "result.show()\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "TORGIafEUvnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. Spark Streaming:\n",
        "\n",
        "  a) Write a Python program to create a Spark Streaming application.\n",
        "\n",
        "   b) Configure the application to consume data from a streaming source (e.g., Kafka or a socket).\n",
        "   \n",
        "   c) Implement streaming transformations and actions to process and analyze the incoming data stream.\n"
      ],
      "metadata": {
        "id": "aZGmqm6jU0HC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#a)\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"StreamingExample\").getOrCreate()\n",
        "\n",
        "# Create a StreamingContext with batch interval of 1 second\n",
        "ssc = StreamingContext(spark.sparkContext, 1)\n",
        "\n",
        "# Set the log level to avoid excessive output\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "SAGEaJ1NU2Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#b)\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.streaming.kafka import KafkaUtils\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"StreamingExample\").getOrCreate()\n",
        "\n",
        "# Create a StreamingContext with batch interval of 1 second\n",
        "ssc = StreamingContext(spark.sparkContext, 1)\n",
        "\n",
        "# Set the log level to avoid excessive output\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "# Configure Kafka parameters\n",
        "kafka_params = {\n",
        "    \"bootstrap.servers\": \"localhost:9092\",  # Kafka broker address\n",
        "    \"group.id\": \"my-group\",  # Consumer group ID\n",
        "    \"auto.offset.reset\": \"latest\"  # Start consuming from the latest offset\n",
        "}\n",
        "\n",
        "# Create a DStream by consuming from a Kafka topic\n",
        "kafka_stream = KafkaUtils.createDirectStream(ssc, [\"my-topic\"], kafka_params)\n",
        "\n",
        "# Start the streaming context\n",
        "ssc.start()\n",
        "ssc.awaitTermination()\n"
      ],
      "metadata": {
        "id": "8AT82Is2U9Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#b)\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"StreamingExample\").getOrCreate()\n",
        "\n",
        "# Create a StreamingContext with batch interval of 1 second\n",
        "ssc = StreamingContext(spark.sparkContext, 1)\n",
        "\n",
        "# Set the log level to avoid excessive output\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "# Create a DStream by reading from a socket (e.g., localhost:9999)\n",
        "socket_stream = ssc.socketTextStream(\"localhost\", 9999)\n",
        "\n",
        "# Process the DStream (see next step)\n",
        "\n",
        "# Start the streaming context\n",
        "ssc.start()\n",
        "ssc.awaitTermination()\n"
      ],
      "metadata": {
        "id": "SKUpkyoYVNWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#c)\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"StreamingExample\").getOrCreate()\n",
        "\n",
        "# Create a StreamingContext with batch interval of 1 second\n",
        "ssc = StreamingContext(spark.sparkContext, 1)\n",
        "\n",
        "# Set the log level to avoid excessive output\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "# Create a DStream by reading from a socket (e.g., localhost:9999)\n",
        "socket_stream = ssc.socketTextStream(\"localhost\", 9999)\n",
        "\n",
        "# Transformations\n",
        "word_counts = socket_stream.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Actions\n",
        "word_counts.pprint()\n",
        "\n",
        "# Start the streaming context\n",
        "ssc.start()\n",
        "ssc.awaitTermination()\n"
      ],
      "metadata": {
        "id": "dUGtj-8MVjBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Spark SQL and Data Source Integration:\n",
        "\n",
        "   a) Write a Python program to connect Spark with a relational database (e.g., MySQL, PostgreSQL).\n",
        "\n",
        "   b)Perform SQL operations on the data stored in the database using Spark SQL.\n",
        "   \n",
        "   c) Explore the integration capabilities of Spark with other data sources, such as Hadoop Distributed File System (HDFS) or Amazon S3.\n"
      ],
      "metadata": {
        "id": "D0SeiQoKVRmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#a)\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"DatabaseExample\").getOrCreate()\n",
        "\n",
        "# Configure MySQL connection properties\n",
        "mysql_properties = {\n",
        "    \"user\": \"your_username\",\n",
        "    \"password\": \"your_password\",\n",
        "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
        "}\n",
        "\n",
        "# Connect to the MySQL database and load a table into a DataFrame\n",
        "df = spark.read.jdbc(\"jdbc:mysql://localhost:3306/database_name\", \"table_name\", properties=mysql_properties)\n",
        "\n",
        "df.show()\n",
        "\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "-W9jqz8RVYgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#b)\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"DatabaseExample\").getOrCreate()\n",
        "\n",
        "# Configure PostgreSQL connection properties\n",
        "postgres_properties = {\n",
        "    \"user\": \"your_username\",\n",
        "    \"password\": \"your_password\",\n",
        "    \"driver\": \"org.postgresql.Driver\"\n",
        "}\n",
        "\n",
        "# Connect to the PostgreSQL database and load a table into a DataFrame\n",
        "df = spark.read.jdbc(\"jdbc:postgresql://localhost:5432/database_name\", \"table_name\", properties=postgres_properties)\n",
        "\n",
        "df.show()\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "ICuJAKdEVtAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#c)\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"DatabaseExample\").getOrCreate()\n",
        "\n",
        "# Configure database connection properties\n",
        "db_properties = {\n",
        "    \"user\": \"your_username\",\n",
        "    \"password\": \"your_password\",\n",
        "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
        "}\n",
        "\n",
        "# Connect to the database and load a table into a DataFrame\n",
        "df = spark.read.jdbc(\"jdbc:mysql://localhost:3306/database_name\", \"table_name\", properties=db_properties)\n",
        "\n",
        "# Register the DataFrame as a temporary view\n",
        "df.createOrReplaceTempView(\"my_table\")\n",
        "\n",
        "# Run SQL queries on the DataFrame\n",
        "result = spark.sql(\"SELECT * FROM my_table WHERE column_name = 'somevalue'\")\n",
        "result.show()\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "TCMitw9LVv7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataSourceExample\").getOrCreate()\n",
        "\n",
        "# Read data from HDFS into a DataFrame\n",
        "df = spark.read.csv(\"hdfs://localhost:9000/path/to/file.csv\", header=True, inferSchema=True)\n",
        "\n",
        "\n",
        "df.show()\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "4wepiWx_V4JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataSourceExample\").getOrCreate()\n",
        "\n",
        "# Read data from Amazon S3 into a DataFrame\n",
        "df = spark.read.csv(\"s3a://bucket_name/path/to/file.csv\", header=True, inferSchema=True)\n",
        "\n",
        "df.show()\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "zekIW6_xV6Yx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}