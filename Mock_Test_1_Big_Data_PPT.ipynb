{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0NDIC8YBmViQHgO5GzsTy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayant3297/Pre_Placement_Training/blob/main/Mock_Test_1_Big_Data_PPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a PySpark code to read a CSV file named \"employees.csv\" containing the following columns: \"employee_id\", \"name\", \"age\", \"department\". Display the top 10 records from the DataFrame."
      ],
      "metadata": {
        "id": "-5It-DypwA77"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atynrrgJ_hVZ"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ReadCSV\").getOrCreate()\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Display the top 10 records\n",
        "df.show(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Given a PySpark DataFrame named \"sales_data\" with columns \"product_name\" and \"revenue\", write a code to calculate the total revenue for each product and display the result in descending order."
      ],
      "metadata": {
        "id": "SbRBcAG1wJIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "#SparkSession\n",
        "spark = SparkSession.builder.appName(\"TotalRevenue\").getOrCreate()\n",
        "\n",
        "# Calculate total revenue for each product\n",
        "total_revenue_df = sales_data.groupBy(\"product_name\").agg(sum(\"revenue\").alias(\"total_revenue\"))\n",
        "\n",
        "# Sort the result in descending order\n",
        "sorted_df = total_revenue_df.orderBy(desc(\"total_revenue\"))\n",
        "\n",
        "# Display the result\n",
        "sorted_df.show()\n"
      ],
      "metadata": {
        "id": "zzM8eb21wJuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a PySpark code to read a JSON file named \"students.json\" containing student records with the following schema: \"name\" (string), \"age\" (integer), \"grade\" (string). Filter the DataFrame to include only students whose age is greater than 18."
      ],
      "metadata": {
        "id": "bVnStZQqwS3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"FilterStudents\").getOrCreate()\n",
        "\n",
        "# Read the JSON file into a DataFrame\n",
        "students_df = spark.read.json(\"students.json\")\n",
        "\n",
        "# Filter the DataFrame to include only students whose age is greater than 18\n",
        "filtered_df = students_df.filter(students_df.age > 18)\n",
        "\n",
        "# Display the filtered DataFrame\n",
        "filtered_df.show()\n"
      ],
      "metadata": {
        "id": "vfkUgBOJwTeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider a PySpark DataFrame named \"transactions\" with columns \"transaction_id\", \"user_id\", and \"amount\". Write a code to calculate the average transaction amount for each user and display the result."
      ],
      "metadata": {
        "id": "eBSU2_0dwV_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"AverageTransaction\").getOrCreate()\n",
        "\n",
        "# Read the DataFrame\n",
        "transactions_df = spark.read.csv(\"transactions.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Calculate the average transaction amount for each user\n",
        "average_amount_df = transactions_df.groupBy(\"user_id\").agg(avg(\"amount\").alias(\"average_amount\"))\n",
        "\n",
        "# Display the result\n",
        "average_amount_df.show()\n"
      ],
      "metadata": {
        "id": "0ydNVDTcwW_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a PySpark DataFrame named \"logs\" with columns \"timestamp\" (timestamp) and \"event\" (string), write a code to count the number of events that occurred in each hour and display the result sorted by the hour."
      ],
      "metadata": {
        "id": "deKM_jv3wZRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import hour\n",
        "from pyspark.sql.types import TimestampType\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"EventCount\").getOrCreate()\n",
        "\n",
        "# Read the DataFrame\n",
        "logs_df = spark.read.csv(\"logs.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Convert timestamp column to timestamp type\n",
        "logs_df = logs_df.withColumn(\"timestamp\", logs_df[\"timestamp\"].cast(TimestampType()))\n",
        "\n",
        "# Extract the hour from the timestamp column\n",
        "logs_df = logs_df.withColumn(\"hour\", hour(logs_df[\"timestamp\"]))\n",
        "\n",
        "# Count the number of events per hour\n",
        "event_count_df = logs_df.groupBy(\"hour\").count().orderBy(\"hour\")\n",
        "\n",
        "# Display the result\n",
        "event_count_df.show()\n"
      ],
      "metadata": {
        "id": "H9gosXkgwZ2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve all the customers from the \"Customers\" table whose age is greater than 25 and have made at least one purchase."
      ],
      "metadata": {
        "id": "CaojS1B7wczl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SELECT *\n",
        "FROM Customers\n",
        "WHERE age > 25\n",
        "  AND customer_id IN (\n",
        "    SELECT customer_id\n",
        "    FROM Purchases\n",
        "    GROUP BY customer_id\n",
        "    HAVING COUNT(*) > 0\n",
        "  );"
      ],
      "metadata": {
        "id": "5FRgcBVVwdlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Find the total number of orders placed by each customer and display the results in descending order of the number of orders."
      ],
      "metadata": {
        "id": "HVhxXf9pwf_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SELECT customer_id, COUNT(*) AS total_orders\n",
        "FROM Orders\n",
        "GROUP BY customer_id\n",
        "ORDER BY total_orders DESC;\n"
      ],
      "metadata": {
        "id": "zADIN4bQwgnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Retrieve the names of all products that are currently out of stock from the \"Products\" table."
      ],
      "metadata": {
        "id": "S_-kbozUwixe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SELECT name\n",
        "FROM Products\n",
        "WHERE stock_quantity = 0;\n"
      ],
      "metadata": {
        "id": "XjA4SLvawjWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the average price of all products in each category and display the results along with the category name."
      ],
      "metadata": {
        "id": "oAUCJ9hCwlzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SELECT c.category_name, AVG(p.price) AS average_price\n",
        "FROM Products p\n",
        "JOIN Categories c ON p.category_id = c.category_id\n",
        "GROUP BY c.category_name;\n"
      ],
      "metadata": {
        "id": "O9n2yEpWwnVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve the top 5 customers who have spent the highest total amount on purchases."
      ],
      "metadata": {
        "id": "t_mrs9-jwphc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SELECT c.customer_name, SUM(o.total_amount) AS total_spent\n",
        "FROM Customers c\n",
        "JOIN Orders o ON c.customer_id = o.customer_id\n",
        "GROUP BY c.customer_name\n",
        "ORDER BY total_spent DESC\n",
        "LIMIT 5;\n"
      ],
      "metadata": {
        "id": "AvxzkJu7wqFJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}