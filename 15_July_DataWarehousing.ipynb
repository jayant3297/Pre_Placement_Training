{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNYPwAVOvNAUAimkVu8+5x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayant3297/Pre_Placement_Training/blob/main/15_July_DataWarehousing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOPIC: Data Warehousing Fundamentals\n",
        "   1. Design a data warehouse schema for a retail company that includes dimension tables for products, customers, and time. Implement the schema using a relational database management system (RDBMS) of your choice.\n",
        "   2. Create a fact table that captures sales data, including product ID, customer ID, date, and sales amount. Populate the fact table with sample data.\n",
        "   3. Write SQL queries to retrieve sales data from the data warehouse, including aggregations and filtering based on different dimensions.\n"
      ],
      "metadata": {
        "id": "rIDpoDXw4wch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Soltuion\n",
        "\n",
        "1. Dimension tables:\n",
        "\n",
        "Products: product_id (primary key), product_name, category, price\n",
        "Customers: customer_id (primary key), customer_name, address, email\n",
        "Time: date_id (primary key), date, day, month, year, quarter\n",
        "Relational database management system (RDBMS): PostgreSQL\n",
        "\n",
        "Creating a fact table and populating it with sample data:\n",
        "Fact table:\n",
        "\n",
        "Sales: sale_id (primary key), product_id (foreign key to Products), customer_id (foreign key to Customers), date_id (foreign key to Time), sales_amount, quantity_sold"
      ],
      "metadata": {
        "id": "ueJ3zpg58vZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the fact table in PostgreSQL:"
      ],
      "metadata": {
        "id": "dOpcyX8o9OdV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmUhcWRZ3Cfd"
      },
      "outputs": [],
      "source": [
        "CREATE TABLE sales (\n",
        "    sale_id SERIAL PRIMARY KEY,\n",
        "    product_id INTEGER,\n",
        "    customer_id INTEGER,\n",
        "    date_id INTEGER,\n",
        "    sales_amount DECIMAL(10,2),\n",
        "    quantity_sold INTEGER,\n",
        "\n",
        "    CONSTRAINT fk_product FOREIGN KEY (product_id) REFERENCES products (product_id),\n",
        "    CONSTRAINT fk_customer FOREIGN KEY (customer_id) REFERENCES customers (customer_id),\n",
        "    CONSTRAINT fk_date FOREIGN KEY (date_id) REFERENCES time (date_id)\n",
        ");\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INSERT INTO sales (product_id, customer_id, date_id, sales_amount, quantity_sold)\n",
        "VALUES\n",
        "    (1, 1, 1, 100.50, 2),\n",
        "    (2, 1, 2, 50.75, 1),\n",
        "    (3, 2, 1, 75.00, 3);"
      ],
      "metadata": {
        "id": "Vt3SfIDv9UfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Writing SQL queries to retrieve sales data from the data warehouse:\n",
        "Aggregation query to calculate total sales amount by product category:"
      ],
      "metadata": {
        "id": "imikhJOs9a5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SELECT p.category, SUM(s.sales_amount) AS total_sales_amount\n",
        "FROM sales s\n",
        "JOIN products p ON s.product_id = p.product_id\n",
        "GROUP BY p.category;\n"
      ],
      "metadata": {
        "id": "WPBxag1T9ea6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtering query to retrieve sales data for a specific customer:"
      ],
      "metadata": {
        "id": "4MA6KuXx9hKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SELECT p.product_name, s.sales_amount, t.date\n",
        "FROM sales s\n",
        "JOIN products p ON s.product_id = p.product_id\n",
        "JOIN time t ON s.date_id = t.date_id\n",
        "WHERE s.customer_id = 1;\n"
      ],
      "metadata": {
        "id": "-oiCquZq9jBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOPIC: ETL and Data Integration\n",
        "  1. Design an ETL process using a programming language (e.g., Python) to extract data from a source system (e.g., CSV files), transform it by applying certain business rules or calculations, and load it into a data warehouse.\n",
        "  2. Implement the ETL process by writing code that performs the extraction, transformation, and loading steps.\n"
      ],
      "metadata": {
        "id": "sDGrSY0f5alk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Designing the ETL process:\n",
        "Extraction: Extract data from CSV files.\n",
        "Transformation: Apply business rules or calculations on the extracted data.\n",
        "Loading: Load the transformed data into a data warehouse.\n",
        "\n",
        "Implementing the ETL process in Python:"
      ],
      "metadata": {
        "id": "hkdOcFP_9q_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import psycopg2\n",
        "\n",
        "# Extract data from CSV files\n",
        "def extract_data(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        csv_reader = csv.reader(file)\n",
        "        next(csv_reader)  # Skip header row\n",
        "        for row in csv_reader:\n",
        "            data.append(row)\n",
        "    return data\n",
        "\n",
        "# Transform data by applying business rules or calculations\n",
        "def transform_data(data):\n",
        "    transformed_data = []\n",
        "    for row in data:\n",
        "        # Apply transformations\n",
        "        transformed_row = [row[0], float(row[1]) * 1.1]  #increase value by 10%\n",
        "        transformed_data.append(transformed_row)\n",
        "    return transformed_data\n",
        "\n",
        "# Load transformed data into a data warehouse (PostgreSQL)\n",
        "def load_data(data):\n",
        "    conn = psycopg2.connect(\n",
        "        host='localhost',\n",
        "        database='mydatabase',\n",
        "        user='myuser',\n",
        "        password='mypassword'\n",
        "    )\n",
        "    cur = conn.cursor()\n",
        "\n",
        "# Create a table\n",
        "    cur.execute('''\n",
        "        CREATE TABLE IF NOT EXISTS transformed_data (\n",
        "            id SERIAL PRIMARY KEY,\n",
        "            field1 VARCHAR(255),\n",
        "            field2 NUMERIC\n",
        "        )\n",
        "    ''')\n",
        "    conn.commit()\n",
        "\n",
        "# Insert data into the table\n",
        "    for row in data:\n",
        "        cur.execute('INSERT INTO transformed_data (field1, field2) VALUES (%s, %s)', row)\n",
        "    conn.commit()\n",
        "\n",
        "# Close the database connection\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "hs4_oNIa5bdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function\n",
        "def etl_process():\n",
        "    # Extract data from CSV files\n",
        "    data = extract_data('data.csv')\n",
        "\n",
        "    # Transform data\n",
        "    transformed_data = transform_data(data)\n",
        "\n",
        "    # Load transformed data into the data warehouse\n",
        "    load_data(transformed_data)\n",
        "\n",
        "    print(\"ETL completed.\")\n",
        "\n",
        "# Run the ETL process\n",
        "etl_process()\n"
      ],
      "metadata": {
        "id": "Kf9LU6rD-Z3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "TOPIC: Dimensional Modeling and Schemas\n",
        "   1. Design a star schema for a university database, including a fact table for student enrollments and dimension tables for students, courses, and time. Implement the schema using a database of your choice.\n",
        "   2. Write SQL queries to retrieve data from the star schema, including aggregations and joins between the fact table and dimension tables.\n"
      ],
      "metadata": {
        "id": "FARlQEn55e77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Designing a star schema for a university database:\n",
        "\n",
        "Fact table:\n",
        "\n",
        "Enrollments: enrollment_id (primary key), student_id (foreign key to Students), course_id (foreign key to Courses), date_id (foreign key to Time)\n",
        "\n",
        "Dimension tables:\n",
        "\n",
        "Students: student_id (primary key), student_name, student_major.\n",
        "\n",
        "Courses: course_id (primary key), course_name, course_department.\n",
        "\n",
        "Time: date_id (primary key), date, day, month, year.\n",
        "\n",
        "Relational database management system (RDBMS): MySQL\n"
      ],
      "metadata": {
        "id": "jLRL7zAA-7eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating tables\n",
        "\n",
        "CREATE TABLE students (\n",
        "    student_id INT PRIMARY KEY,\n",
        "    student_name VARCHAR(255),\n",
        "    student_major VARCHAR(255)\n",
        ");\n",
        "\n",
        "CREATE TABLE courses (\n",
        "    course_id INT PRIMARY KEY,\n",
        "    course_name VARCHAR(255),\n",
        "    course_department VARCHAR(255)\n",
        ");\n",
        "\n",
        "CREATE TABLE time (\n",
        "    date_id INT PRIMARY KEY,\n",
        "    date DATE,\n",
        "    day INT,\n",
        "    month INT,\n",
        "    year INT\n",
        ");\n",
        "\n",
        "CREATE TABLE enrollments (\n",
        "    enrollment_id INT PRIMARY KEY,\n",
        "    student_id INT,\n",
        "    course_id INT,\n",
        "    date_id INT,\n",
        "    FOREIGN KEY (student_id) REFERENCES students (student_id),\n",
        "    FOREIGN KEY (course_id) REFERENCES courses (course_id),\n",
        "    FOREIGN KEY (date_id) REFERENCES time (date_id)\n",
        ");\n"
      ],
      "metadata": {
        "id": "28ACst3V5fpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add data to the table\n",
        "\n",
        "INSERT INTO students (student_id, student_name, student_major)\n",
        "VALUES\n",
        "    (1, 'John Doe', 'Computer Science'),\n",
        "    (2, 'Jane Smith', 'Mathematics'),\n",
        "    (3, 'Alice Johnson', 'Physics');\n",
        "\n",
        "\n",
        "INSERT INTO courses (course_id, course_name, course_department)\n",
        "VALUES\n",
        "    (1, 'Introduction to Computer Science', 'Computer Science'),\n",
        "    (2, 'Calculus I', 'Mathematics'),\n",
        "    (3, 'Physics 101', 'Physics');\n",
        "\n",
        "\n",
        "INSERT INTO time (date_id, date, day, month, year)\n",
        "VALUES\n",
        "    (1, '2022-01-01', 1, 1, 2022),\n",
        "    (2, '2022-01-02', 2, 1, 2022),\n",
        "    (3, '2022-01-03', 3, 1, 2022);\n",
        "\n"
      ],
      "metadata": {
        "id": "c3WTiMsy_ZSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Writing SQL queries to retrieve data from the star schema:\n",
        "#Aggregation query to calculate the total number of enrollments by course\n",
        "\n",
        "SELECT c.course_name, COUNT(*) AS total_enrollments\n",
        "FROM enrollments e\n",
        "JOIN courses c ON e.course_id = c.course_id\n",
        "GROUP BY c.course_name;\n"
      ],
      "metadata": {
        "id": "1B6PA8-9_xBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Join query to retrieve the student name, course name, and enrollment date:\n",
        "\n",
        "SELECT s.student_name, c.course_name, t.date\n",
        "FROM enrollments e\n",
        "JOIN students s ON e.student_id = s.student_id\n",
        "JOIN courses c ON e.course_id = c.course_id\n",
        "JOIN time t ON e.date_id = t.date_id;\n"
      ],
      "metadata": {
        "id": "M4l0nIIQAZvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOPIC: Performance Optimization and Querying\n",
        "\n",
        "    1. Scenario: You need to improve the performance of your data loading process in the data warehouse. Write a Python script that implements the following optimizations:\n",
        "\n",
        "      a)Utilize batch processing techniques to load data in bulk instead of individual row insertion.\n",
        "\n",
        "      b)  Implement multi-threading or multiprocessing to parallelize the data loading process.\n",
        "      c)  Measure the time taken to load a specific amount of data before and after implementing these optimizations.\n"
      ],
      "metadata": {
        "id": "Kbm38Prj5lBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Python script that implements the mentioned optimizations to improve the performance of the data loading process in a data warehouse\n",
        "\n",
        "import time\n",
        "import threading\n",
        "from multiprocessing import Pool\n",
        "\n",
        "# Function to simulate data loading process\n",
        "def load_data(data)\n",
        "    time.sleep(0.1)  # Simulating a delay of 0.1 seconds\n",
        "\n",
        "# Function to load data in bulk using batch processing\n",
        "def load_data_in_bulk(data, batch_size):\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        load_data(batch)\n",
        "\n",
        "# Function to load data in parallel using multi-threading\n",
        "def load_data_multi_threading(data, num_threads):\n",
        "    threads = []\n",
        "    for i in range(num_threads):\n",
        "        thread = threading.Thread(target=load_data, args=(data,))\n",
        "        threads.append(thread)\n",
        "        thread.start()\n",
        "\n",
        "    for thread in threads:\n",
        "        thread.join()\n",
        "\n",
        "# Function to load data in parallel using multiprocessing\n",
        "def load_data_multiprocessing(data, num_processes):\n",
        "    pool = Pool(processes=num_processes)\n",
        "    pool.map(load_data, [data] * num_processes)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "# Measure the time taken to load data\n",
        "def measure_loading_time(data, num_iterations):\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # Call the optimized data loading function here\n",
        "        load_data_in_bulk(data, batch_size=1000)\n",
        "        # load_data_multi_threading(data, num_threads=4)\n",
        "        # load_data_multiprocessing(data, num_processes=4)\n",
        "\n",
        "    end_time = time.time()\n",
        "    loading_time = end_time - start_time\n",
        "\n",
        "    return loading_time\n",
        "\n",
        "# Generate sample data for loading\n",
        "data = [i for i in range(10000)]  # Sample data to be loaded\n",
        "num_iterations = 10  # Number of times to repeat the loading process\n",
        "\n",
        "# Measure the loading time before implementing optimizations\n",
        "loading_time_before = measure_loading_time(data, num_iterations)\n",
        "\n",
        "# Measure the loading time after implementing optimizations\n",
        "loading_time_after = measure_loading_time(data, num_iterations)\n",
        "\n",
        "# Print the loading times\n",
        "print(\"Loading time before optimization:\", loading_time_before)\n",
        "print(\"Loading time after optimization:\", loading_time_after)\n"
      ],
      "metadata": {
        "id": "KSOrpdAO5qpN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}